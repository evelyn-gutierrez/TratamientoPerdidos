---
title: "Tratamiento de valores perdidos con R"
author:
  - Evelyn Gutierrez^[egutierreza@pucp.edu.pe]
  - Vilma Romero^[vromero@uni.pe]
date: "September, 2021"
output:
  rmdformats::robobook:
    highlight: tango
    number_sections: true
    code_folding: show
    code_download: TRUE
  html_document:
    toc: yes
  pdf_document: 
    number_sections: yes
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
```



# Introducción.


```{r echo=FALSE}
knitr::include_url("files/3_Tratamiento de datos perdidos.pdf", height = "600px")
```



# Laboratorio en R.

En esta sesión, realizaremos la imputación de datos perdidos utilizando técnicas básicas y por vecinos más cercanos. 

Requerimos instalar los siguientes paquetes: 

* `Hmisc`
* `VIM`
* `mice`
* `DMwR`


Para `DMwR`, utilizar: `remotes::install_github("cran/DMwR")`


# Exploración de valores perdidos.
  
  

<br>

## Exploración básica.

**Caso 1: Notas.**
  
Iniciamos este ejemplo, creando un data.frame notas con alguna nota faltante.
  
```{r}
notas <- data.frame(nombre = c("Jesus", "Carla", "Rodrigo", "Javier"),
                    nota = c(12, 15, 13, NA))
notas
```

Exploramos visualmente el número de valores perdidos por variable: solo existe un valor aleatorio.

Finalmente, seleccionar los datos completos con `complete.cases`.
```{r}
notas_comp <- notas[complete.cases(notas),]
notas_comp
```
 

<br>

## Visualizaciones

 
**Caso 2: Dataset sleep**  



Utilizaremos el conjunto de datos *sleep* del paquete VIM para realizar la exploración de valores perdidos en R.
 
- Instalación:

Necesitamos instalar el paquete VIM con el siguiente código en la consola: `install.packages("VIM")`.

Luego, cargamos los datos de `sleep` y vemos las primeras filas del dataset utilizando el siguiente código:


```{r}
# Carga los datos.
data(sleep, package = "VIM")

# Vemos las 6 primeras filas.
head(sleep)
```


Comprobaremos que el dataset "sleep" ahora aparece también en su **Environment** en RStudio. 


Iniciamos la **exploración inicial** de este nuevo dataset con alguno de los siguientes comandos básicos:

```{r}
str(sleep)
dplyr::glimpse(sleep)
summary(sleep)
```


En todos ellos observaremos una primera vista de los datos. Notaremos además, que existen valores NA, datos perdidos.
La primera pregunta que nos hacemos es:


> ¿Cuántos datos están con valores NA en este dataset?


Para contar el número de valores perdidos por variable podemos usar este cálculo con la función *apply* que cuenta el número de valores perdidos (valores NA para R) por columna. 

```{r}
apply(sleep, 2, function(x){sum(is.na(x))})
```
 
 
Ahora podemos responder lo siguiente `r emo::ji("teacher")`: 

* ¿Cuántos valores perdidos hay en cada variable?
* ¿Qué variables tienen valores perdidos?
* ¿Qué variables tienen más valores perdidos? `r emo::ji("raised")`
 

<br>


Continuamos explorando los valores perdidos analizando el **patrón de valores perdidos** distribuidos **en las diferentes variables** del conjunto de datos (dataset). Esto nos ayudará a entender mejor nuestros datos. 

Lo hacemos utilizando la función *md.pattern* y *md.pairs* del paquete **MICE**.

```{r out.width='80%'}
mice::md.pattern(sleep, rotate.names=TRUE)
mice::md.pairs(sleep)
```

En estos gráficos y tablas observamos las diferentes combinaciones de valores perdidos que tenemos para nuestras variables. Ahora, podemos responder las siguiente preguntas: 

* ¿Cuantas observaciones no tienen nigún valor perdido?
* ¿Cuantas observaciones no tienen nigún valor perdido?

Visualización de datos perdidos
```{r}
sleep_aggr <- VIM::aggr(sleep, col = mice::mdc(1:2), numbers = TRUE, 
                        sortVars = TRUE, labels = names(sleep),
                        cex.axis= 0.7, gap = 3,
                        ylab = c("Proporción de Pérdida",
                                 "Patrón de Pérdida"))
```

Distribución de observaciones completas e incompletas por pares de variables
```{r}
VIM::marginplot(sleep[ , c(3, 7)], pch = 19)
VIM::marginplot(sleep[ , c(3, 7)], col = c("blue", "red", "orange"), pch = 20)
```

Descripción:

* Puntos azules (diagrama de dispersión): individuos con ambos valores de las variables.
* Boxplots azules: boxplots de los valores no perdidos de cada variable

* Puntos rojos (Eje X: NonD): individuos con valores perdidos en Gest pero observados en NonD.
* Puntos rojos (Eje Y: Gest): individuos con valores perdidos en NonD pero observados en Gest.
* Boxplots rojos: Representan la distribución marginal de los puntos rojos.

Nota: Si los datos perdidos son completamente aleatorios se espera que
los boxplots rojos y azules sean idénticos


\newpage


# Imputación Univariada

```{r eval=FALSE, warning=FALSE, include=FALSE}

# Imputación Univariada

```

## Con la media.

instalamos la librería `Hmisc` para realizar imputaciones básicas. La instalación, la realizaremos utilizando el siguiente comando en la consola: `install.packages("Hmisc")`.

Luego de completada la instalación, comprobamos cargando el paquete.


```{r warning=FALSE}
library(Hmisc)
```


Si no tenemos mayor información, utilizaremos la media como valor de imputación.
Es una imputación rápida, simple y sencilla.

```{r}
notas$nota_imp <- with(notas, impute(nota, mean))
notas
```

***

## Con valor aleatorio.

Utilizamos un valor aleatorio como valor de imputación: Se selecciona aleatoriamente a partir de los valores no perdidos. Simple y útil en caso de MCAR.

```{r}
notas$nota_imp <- with(notas, impute(nota, 'random'))
notas
```

***

## Con un valor específico.

Si tenemos información específica, o resulta conveniente, podemos imputar los datos perdidos con un valor específico.


```{r}
notas$nota_imp <- with(notas, impute(nota, 99))
notas
```

***

## Manualmente

Por ultimo, la imputación puede realizarse sin el paquete Hmisc de la siguiente manera:

```{r}
notas$nota[is.na(notas$nota)] <- mean(notas$nota, na.rm = T)
notas
```


<br>

# Imputación Multivariada

```{r eval=FALSE, warning=FALSE, include=FALSE}

# Imputación Multivariada

```

## Por regresión lineal.

Con la librería mice. Esta librería sirve para imputación múltiple pero podemos usarla también para imputación simple si definimos *m=1*.  

```{r}
library(mice) 
imp <- mice(sleep, method = "norm.predict", m = 1, maxit=1) # Impute data
imp_reg <- complete(imp)
```

Para missings en variables categorícas se puede utilizar regresión logistica con el argumento `method="logreg"`. Ejemplo: 
`mice(nhanes2, meth = c("sample", "norm.predict", "logreg", "norm.predict"))`

Para ver otros métodos, podemos ver la documentación de la función mice escribiendo `?mice::mice` en la consola.

<br>

## Por K vecinos más cercanos.
 
Aplicamos vecions más cercanos y guardamos los resultados en sleep_imp

```{r}
library(DMwR)
sleep_imp <- DMwR::knnImputation(sleep)
#View(sleep_imp)
summary(sleep_imp)
```

¿Hay datos perdidos ahora? 

```{r}
apply(sleep_imp, 2, function(x){sum(is.na(x))})
```


## Por bosques aleatorios.

```{r}
library(missForest)
sleep_imp_rf <- missForest(sleep)
print(sleep_imp$NonD, digits = 3)
```

## MICE

> MICE: *Multivariate Imputation by Chained Equations*

Utilizaremos la metodología MICE: Multivariate Imputation by Chained Equations para realizar imputación multivariada.

La imputación con MICE puede ser simple o múltiple. Simple si solo se imputa el dataset inicial; y múltiple cuando se crean multiples datasets con diferentes imputaciones.


```{r}
library(VIM)
library(mice)
```
 

# Imputación Múltiple

```{r eval=FALSE, warning=FALSE, include=FALSE}

# Imputación Multiple

```

## MICE

Utilizamos el paquete MICE: Imputación Multivariada por Chained Equations para realizar la imputación múltiple.

```{r}
library(mice)
```

La imputación se realiza con estas líneas de código:

```{r}
imp1 <- mice(sleep, m = 5, seed = 2)
imp1
```

El argumento m=5 indica que se crearan 5 datasets de imputaciones.


Verificamos el métodos de imputación utilizado:
```{r}
imp1$method
```

Como vemos, se usó el método pmm (Predictive mean matching): Un método de imputación semi-parámetrico usado por defecto para variables continuas. 
  - Selecciona un grupo de candidatos vecino similares y cercanos, y toma uno aleatoriamente como donador.


<br>


Imputaciones para una variable en particular. Veamos el objeto **imp1**, que tiene una lista de imputados **imp** con un set de imputados para la columna NonD 

```{r}
head(imp1$imp$NonD)
```

Notemos que cada columna representa a un set de valores imputados para una variable.

## Visualización.

Estos gráficos nos servirán para revisar si las imputaciones realizadas son muy variables entre diferentes datasets.

- El primer gráfico muestra los valores perdidos para la variable en el eje Y: **Gest**.

- Se muestran 6 cuadros correspondientes a 
  
    - La data original y 
    - Los 5 dataset construidos con la imputación multiple. 
    
- En **rojo** están las observaciones imputadas para la variable Gest (variable del eje Y); y en **azul**, todas las demás observaciones. 

- Los puntos azules son los datos observados y además imputaciones realizadas en la variable **NonD** (variable del eje X). 

```{r}
library(lattice)
xyplot(imp1, Gest ~ NonD | .imp, pch = 20, cex = 1.4)
```

<br>


- En el siguiente gráfico observamos el mismo tipo de diagrama. Esta vez **enfocado** en el análisis de otra variable: **NonD** (Note la diferencia en la formula utilizada *NonD ~ Gest*). 


- A partir de los **puntos rosados** en los diferentes cuadros, se observan las variaciones en las imputaciones para **NonD** en los diferentes datasets contruídos durante la imputación múltiple. 


- Notemos a partir de estos gráficos que se están imputando valores fuera de la nube de puntos creada entre estas dos variables. Aunque podría suceder. 


```{r}
xyplot(imp1, NonD ~ Gest | .imp, pch = 20, cex = 1.4)
```


<br>

Finalmente, para observar los datos de las 5 imputaciones en un solo gráfico, tenemos el siguiente código.

**Para la variable Gest**

```{r out.width='70%'}
xyplot(imp1, Gest ~ NonD, pch = 18)
```

**Para la variable NonD**

```{r out.width='70%'}
xyplot(imp1, NonD ~ Gest, pch = 18)
```

<br>


Ademas, si queremos incluir una tercera variable al análisis podemos observarla cambiando la formula como el siguiente código.

```{r out.width='90%'}
xyplot(imp1, Gest ~ NonD + Span , pch = 18)
```

- Veremos la relación de la variable Gest con Span además de con NonD.

- Los **puntos rosados** son los valores imputados.


<br>


Finalmente, utilizaremos un gráfico para la densidad de las observaciones imputadas en cada dataset.

- Esto nos mostrará si las diferentes imputaciones están concentradas en los mismos valores o si cambian entre diferentes datasets. 

- Cada densidad está representada en líneas de color rosado y representan la densidad para las imputaciones en uno de los 5 datasets de la imputación múltiple.

- La densidad en color celeste representa la densidad de los valores observados.


```{r}
densityplot(imp1)
```

- Este gráfico compara la **densidad de los datos observados versus la densidad de los datos imputados**. Se espera que las líneas sean similares pero no idénticos.

- Encontrar diferencias entre las diferentes imputaciones indica que las **imputaciones varían** entre diferentes datasets.


<br>

**Streeplot**

- El último gráfico llamado **stripplot** muestra la distribución de cada variable y sus valores imputados en los multiples datasets. 

- Es **otra forma** de ver la **distribución de los imputados** en las diferentes muestras. 

```{r}
stripplot(imp1, pch = 20)
```


# Modelamiento  


## Casos completos

El caso más simple y rápido será utilizando solo los datos completos.
En este caso, omitimos las fila con valores perdidos y construimos nuestro modelo.

```{r}
ajuste_cc <- lm(BodyWgt ~ Sleep + BrainWgt, 
                data = na.omit(sleep))
summary(ajuste_cc)
```

## Imputación simple.

Despues de una imputación simple, el resultado es un dataset con el mismo número de filas y columna pero con todos los datos llenos con algún valor imputado. 
Al realizar el modelamiento, se utilizan los resultados de la imputación realizada para entrenar el modelo. 

```{r}
ajuste_cc <- lm(BodyWgt ~ Sleep + BrainWgt, data = imp_reg)
summary(ajuste_cc)
```

- Nota: Si deseamos utilizar la imputación para nuestra data de validación, tenemos que aplicar la metodología y modelos creados para la imputación a partir de la data de entrenamiento. 

- No deben realizarse modelos para imputaciones con los datos de validación sino podríamos sesgar la evaluación del modelo en la data de entrenamiento. 


## Imputación múltiple.


- Luego de una imputación multiple, el entrenamiento del modelo debe realizarse en cada uno los múltiples datasets imputados. 

- Multiples modelos serán entrenados a partir de los datasets. Es nuestra tarea evaluar la **variabilidad de los modelos** en los diferentes conjuntos de datos y analizar el performance conjunto de todos ellos. (Cuando los modelos sirven para entender el problema, es mejor buscar características estables entre diferentes imputaciones.)

- Ejemplo en R: uso de regresión lineal para los múltiples datasets imputados. El resultados del modelo es el siguiente: 

```{r}
ajuste_imp <- with(imp1, lm( BodyWgt ~ Sleep + BrainWgt))
summary(ajuste_imp)
```

Note que es posible utilizar cualquier otra función en lugar de `lm()`.
El resultado será una lista de modelos para cada dataset.

Performance del modelo en las 5 diferentes imputaciones con la librería *performance*.  

```{r}
library(performance)
lapply(ajuste_imp$analyses, performance)
```



Finalmente, el análisis de resultados se realizará combinando los resultados de cada modelos. 
En nuestro caso, se juntan los coeficientes y errores estándares de los 5 modelos de regresión.

```{r}
ajuste_comb <- pool(ajuste_imp)
summary(ajuste_comb)
pool.r.squared(ajuste_imp)
```



Estos resultados definen el modelo final a evaluar con la data de validación. 

<!-- knitr::purl() -->



# Ejercicio

- [Descripción del ejercicio](files/EjercicioImputacion1.pdf)

- [Datos Advertising.csv](files/Advertising.csv)

# Anexos

- [Slides Imputación](files/3_Tratamiento de datos perdidos.pdf)

- [Códigos utilizados en este manual en R](files/GuiaRImputacion_20210904.R)

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE, class.source = "fold-hide"} 
``` 


- [Descripción de columnas del dataset _sleep_](files/sleep_data_names.pdf)
